{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25846b15",
   "metadata": {
    "id": "25846b15"
   },
   "source": [
    "# Treinamento Normal Sem PSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb1b67",
   "metadata": {
    "id": "99bb1b67"
   },
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad765059",
   "metadata": {
    "id": "ad765059",
    "outputId": "234d2edd-48d1-44a9-d42e-fa79fc399a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Validation Loss: 1.2057, Validation Accuracy: 0.5717\n",
      "Epoch 2/30, Validation Loss: 0.9172, Validation Accuracy: 0.6732\n",
      "Epoch 3/30, Validation Loss: 0.8025, Validation Accuracy: 0.7235\n",
      "Epoch 4/30, Validation Loss: 0.6986, Validation Accuracy: 0.7602\n",
      "Epoch 5/30, Validation Loss: 0.6264, Validation Accuracy: 0.7807\n",
      "Epoch 6/30, Validation Loss: 0.6061, Validation Accuracy: 0.7945\n",
      "Epoch 7/30, Validation Loss: 0.5681, Validation Accuracy: 0.8033\n",
      "Epoch 8/30, Validation Loss: 0.5666, Validation Accuracy: 0.8152\n",
      "Epoch 9/30, Validation Loss: 0.5458, Validation Accuracy: 0.8203\n",
      "Epoch 10/30, Validation Loss: 0.5573, Validation Accuracy: 0.8179\n",
      "Epoch 11/30, Validation Loss: 0.5733, Validation Accuracy: 0.8262\n",
      "Epoch 12/30, Validation Loss: 0.5655, Validation Accuracy: 0.8323\n",
      "Epoch 13/30, Validation Loss: 0.5815, Validation Accuracy: 0.8392\n",
      "Epoch 14/30, Validation Loss: 0.6009, Validation Accuracy: 0.8364\n",
      "Epoch 15/30, Validation Loss: 0.5822, Validation Accuracy: 0.8317\n",
      "Epoch 16/30, Validation Loss: 0.6438, Validation Accuracy: 0.8341\n",
      "Epoch 17/30, Validation Loss: 0.6580, Validation Accuracy: 0.8346\n",
      "Epoch 18/30, Validation Loss: 0.6538, Validation Accuracy: 0.8389\n",
      "Epoch 19/30, Validation Loss: 0.6636, Validation Accuracy: 0.8349\n",
      "Epoch 20/30, Validation Loss: 0.6536, Validation Accuracy: 0.8366\n",
      "Epoch 21/30, Validation Loss: 0.6513, Validation Accuracy: 0.8404\n",
      "Epoch 22/30, Validation Loss: 0.6587, Validation Accuracy: 0.8409\n",
      "Epoch 23/30, Validation Loss: 0.6972, Validation Accuracy: 0.8382\n",
      "Epoch 24/30, Validation Loss: 0.7600, Validation Accuracy: 0.8255\n",
      "Epoch 25/30, Validation Loss: 0.7381, Validation Accuracy: 0.8351\n",
      "Epoch 26/30, Validation Loss: 0.6469, Validation Accuracy: 0.8421\n",
      "Epoch 27/30, Validation Loss: 0.7302, Validation Accuracy: 0.8363\n",
      "Epoch 28/30, Validation Loss: 0.6799, Validation Accuracy: 0.8385\n",
      "Epoch 29/30, Validation Loss: 0.7325, Validation Accuracy: 0.8364\n",
      "Epoch 30/30, Validation Loss: 0.6687, Validation Accuracy: 0.8471\n",
      "Treinamento concluído.\n",
      "Avaliação final - Loss: 0.6687, Accuracy: 0.8471\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Função para criar um subconjunto dos dados\n",
    "def create_subset(dataset, subset_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    subset_indices = random.sample(indices, subset_size)\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "\n",
    "# Função para selecionar o modelo sem pré-treinamento\n",
    "def select_model(architecture):\n",
    "    if architecture == 'alexnet':\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'vgg11':\n",
    "        model = models.vgg11(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    else:\n",
    "        raise ValueError(\"Arquitetura não suportada: escolha 'alexnet', 'vgg11' ou 'resnet18'\")\n",
    "    return model.to(device)\n",
    "\n",
    "# Transformações de dados com normalizações para modelos pré-treinados\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Reduzir a quantidade de dados usados\n",
    "train_subset_size = 50000  # Número ainda mais reduzido de exemplos de treino\n",
    "test_subset_size = 10000    # Número ainda mais reduzido de exemplos de teste\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_subset = create_subset(trainset, train_subset_size)\n",
    "trainloader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_subset = create_subset(testset, test_subset_size)\n",
    "testloader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "# Carregar o conjunto de dados CIFAR-10\n",
    "#trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criar DataLoader\n",
    "#trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "#testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Função para treinamento\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(trainloader)\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    print('Treinamento concluído.')\n",
    "    return model\n",
    "\n",
    "# Função para avaliação\n",
    "def evaluate_model(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Definir arquitetura e otimizador\n",
    "architecture = 'alexnet'  # Escolha entre 'alexnet', 'vgg11' ou 'resnet18'\n",
    "optimizer_choice = 'Adam'  # Escolha entre 'AdaGrad', 'RMSProp' ou 'Adam'\n",
    "\n",
    "model = select_model(architecture)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Selecionar o otimizador\n",
    "if optimizer_choice == 'AdaGrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "elif optimizer_choice == 'RMSProp':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "elif optimizer_choice == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "else:\n",
    "    raise ValueError(\"Otimizador não suportado: escolha 'AdaGrad', 'RMSProp' ou 'Adam'\")\n",
    "\n",
    "# Treinar o modelo\n",
    "num_epochs = 30\n",
    "trained_model = train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Avaliação final\n",
    "final_loss, final_accuracy = evaluate_model(trained_model, criterion)\n",
    "print(f'Avaliação final - Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "torch.save(trained_model.state_dict(), f'trained_model_{architecture}_{optimizer_choice}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bcb49",
   "metadata": {
    "id": "f75bcb49"
   },
   "source": [
    "## VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440553d4",
   "metadata": {
    "id": "440553d4",
    "outputId": "ba4ea28c-57f7-4e19-9dee-dbc3583765bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Validation Loss: 1.1068, Validation Accuracy: 0.6130\n",
      "Epoch 2/30, Validation Loss: 0.7204, Validation Accuracy: 0.7534\n",
      "Epoch 3/30, Validation Loss: 0.6741, Validation Accuracy: 0.7697\n",
      "Epoch 4/30, Validation Loss: 0.6617, Validation Accuracy: 0.7956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Treinar o modelo\u001b[39;00m\n\u001b[0;32m    130\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m--> 131\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_model(model, criterion, optimizer, num_epochs)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Avaliação final\u001b[39;00m\n\u001b[0;32m    134\u001b[0m final_loss, final_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, criterion)\n",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     78\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 81\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     83\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)\n\u001b[0;32m     84\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, criterion)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Função para criar um subconjunto dos dados\n",
    "def create_subset(dataset, subset_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    subset_indices = random.sample(indices, subset_size)\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "\n",
    "# Função para selecionar o modelo sem pré-treinamento\n",
    "def select_model(architecture):\n",
    "    if architecture == 'alexnet':\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'vgg11':\n",
    "        model = models.vgg11(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    else:\n",
    "        raise ValueError(\"Arquitetura não suportada: escolha 'alexnet', 'vgg11' ou 'resnet18'\")\n",
    "    return model.to(device)\n",
    "\n",
    "# Transformações de dados com normalizações para modelos pré-treinados\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Reduzir a quantidade de dados usados\n",
    "train_subset_size = 50000  # Número ainda mais reduzido de exemplos de treino\n",
    "test_subset_size = 10000    # Número ainda mais reduzido de exemplos de teste\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_subset = create_subset(trainset, train_subset_size)\n",
    "trainloader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_subset = create_subset(testset, test_subset_size)\n",
    "testloader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "# Carregar o conjunto de dados CIFAR-10\n",
    "#trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criar DataLoader\n",
    "#trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "#testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Função para treinamento\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(trainloader)\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    print('Treinamento concluído.')\n",
    "    return model\n",
    "\n",
    "# Função para avaliação\n",
    "def evaluate_model(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Definir arquitetura e otimizador\n",
    "architecture = 'vgg11'  # Escolha entre 'alexnet', 'vgg11' ou 'resnet18'\n",
    "optimizer_choice = 'Adam'  # Escolha entre 'AdaGrad', 'RMSProp' ou 'Adam'\n",
    "\n",
    "model = select_model(architecture)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Selecionar o otimizador\n",
    "if optimizer_choice == 'AdaGrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "elif optimizer_choice == 'RMSProp':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "elif optimizer_choice == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "else:\n",
    "    raise ValueError(\"Otimizador não suportado: escolha 'AdaGrad', 'RMSProp' ou 'Adam'\")\n",
    "\n",
    "# Treinar o modelo\n",
    "num_epochs = 30\n",
    "trained_model = train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Avaliação final\n",
    "final_loss, final_accuracy = evaluate_model(trained_model, criterion)\n",
    "print(f'Avaliação final - Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "torch.save(trained_model.state_dict(), f'trained_model_{architecture}_{optimizer_choice}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924c930",
   "metadata": {
    "id": "0924c930"
   },
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fb9d9a",
   "metadata": {
    "id": "06fb9d9a",
    "outputId": "44e1793c-570e-473f-8f65-adf5e88a6a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Validation Loss: 1.1458, Validation Accuracy: 0.5904\n",
      "Epoch 2/30, Validation Loss: 0.8721, Validation Accuracy: 0.6986\n",
      "Epoch 3/30, Validation Loss: 0.7350, Validation Accuracy: 0.7437\n",
      "Epoch 4/30, Validation Loss: 0.6706, Validation Accuracy: 0.7740\n",
      "Epoch 5/30, Validation Loss: 0.7026, Validation Accuracy: 0.7700\n",
      "Epoch 6/30, Validation Loss: 0.7430, Validation Accuracy: 0.7696\n",
      "Epoch 7/30, Validation Loss: 0.8714, Validation Accuracy: 0.7577\n",
      "Epoch 8/30, Validation Loss: 0.8923, Validation Accuracy: 0.7684\n",
      "Epoch 9/30, Validation Loss: 0.7884, Validation Accuracy: 0.7882\n",
      "Epoch 10/30, Validation Loss: 1.1037, Validation Accuracy: 0.7419\n",
      "Epoch 11/30, Validation Loss: 1.1387, Validation Accuracy: 0.7325\n",
      "Epoch 12/30, Validation Loss: 1.1466, Validation Accuracy: 0.7494\n",
      "Epoch 13/30, Validation Loss: 1.2531, Validation Accuracy: 0.7287\n",
      "Epoch 14/30, Validation Loss: 1.1298, Validation Accuracy: 0.7514\n",
      "Epoch 15/30, Validation Loss: 1.0260, Validation Accuracy: 0.7701\n",
      "Epoch 16/30, Validation Loss: 1.3462, Validation Accuracy: 0.7332\n",
      "Epoch 17/30, Validation Loss: 0.8520, Validation Accuracy: 0.7986\n",
      "Epoch 18/30, Validation Loss: 0.8663, Validation Accuracy: 0.8015\n",
      "Epoch 19/30, Validation Loss: 0.9496, Validation Accuracy: 0.7887\n",
      "Epoch 20/30, Validation Loss: 0.9713, Validation Accuracy: 0.7887\n",
      "Epoch 21/30, Validation Loss: 0.9169, Validation Accuracy: 0.7990\n",
      "Epoch 22/30, Validation Loss: 1.0020, Validation Accuracy: 0.7832\n",
      "Epoch 23/30, Validation Loss: 0.9985, Validation Accuracy: 0.7922\n",
      "Epoch 24/30, Validation Loss: 0.9632, Validation Accuracy: 0.7974\n",
      "Epoch 25/30, Validation Loss: 1.0807, Validation Accuracy: 0.7770\n",
      "Epoch 26/30, Validation Loss: 0.8983, Validation Accuracy: 0.8058\n",
      "Epoch 27/30, Validation Loss: 0.9681, Validation Accuracy: 0.7943\n",
      "Epoch 28/30, Validation Loss: 0.9090, Validation Accuracy: 0.8066\n",
      "Epoch 29/30, Validation Loss: 1.0330, Validation Accuracy: 0.7869\n",
      "Epoch 30/30, Validation Loss: 1.1230, Validation Accuracy: 0.7770\n",
      "Treinamento concluído.\n",
      "Avaliação final - Loss: 1.1230, Accuracy: 0.7770\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Função para criar um subconjunto dos dados\n",
    "def create_subset(dataset, subset_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    subset_indices = random.sample(indices, subset_size)\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "\n",
    "# Função para selecionar o modelo sem pré-treinamento\n",
    "def select_model(architecture):\n",
    "    if architecture == 'alexnet':\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'vgg11':\n",
    "        model = models.vgg11(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    else:\n",
    "        raise ValueError(\"Arquitetura não suportada: escolha 'alexnet', 'vgg11' ou 'resnet18'\")\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Transformações de dados com normalizações para modelos pré-treinados\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Reduzir a quantidade de dados usados\n",
    "train_subset_size = 50000  # Número ainda mais reduzido de exemplos de treino\n",
    "test_subset_size =10000    # Número ainda mais reduzido de exemplos de teste\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_subset = create_subset(trainset, train_subset_size)\n",
    "trainloader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_subset = create_subset(testset, test_subset_size)\n",
    "testloader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "# Carregar o conjunto de dados CIFAR-10\n",
    "#trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criar DataLoader\n",
    "#trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "#testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Função para treinamento\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(trainloader)\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    print('Treinamento concluído.')\n",
    "    return model\n",
    "\n",
    "# Função para avaliação\n",
    "def evaluate_model(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Definir arquitetura e otimizador\n",
    "architecture = 'resnet18'  # Escolha entre 'alexnet', 'vgg11' ou 'resnet18'\n",
    "optimizer_choice = 'Adam'  # Escolha entre 'AdaGrad', 'RMSProp' ou 'Adam'\n",
    "\n",
    "model = select_model(architecture)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Selecionar o otimizador\n",
    "if optimizer_choice == 'AdaGrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "elif optimizer_choice == 'RMSProp':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "elif optimizer_choice == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "else:\n",
    "    raise ValueError(\"Otimizador não suportado: escolha 'AdaGrad', 'RMSProp' ou 'Adam'\")\n",
    "\n",
    "# Treinar o modelo\n",
    "num_epochs = 30\n",
    "trained_model = train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Avaliação final\n",
    "final_loss, final_accuracy = evaluate_model(trained_model, criterion)\n",
    "print(f'Avaliação final - Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "torch.save(trained_model.state_dict(), f'trained_model_{architecture}_{optimizer_choice}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc6204",
   "metadata": {
    "id": "f2fc6204"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc33022",
   "metadata": {
    "id": "ecc33022"
   },
   "source": [
    "## MobileNet_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a12b79",
   "metadata": {
    "id": "54a12b79",
    "outputId": "d54135cb-8f9e-46aa-e80d-45fbe51f325b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Validation Loss: 1.4482, Validation Accuracy: 0.4653\n",
      "Epoch 2/30, Validation Loss: 1.2185, Validation Accuracy: 0.5551\n",
      "Epoch 3/30, Validation Loss: 1.0300, Validation Accuracy: 0.6331\n",
      "Epoch 4/30, Validation Loss: 0.9556, Validation Accuracy: 0.6583\n",
      "Epoch 5/30, Validation Loss: 0.8802, Validation Accuracy: 0.6893\n",
      "Epoch 6/30, Validation Loss: 0.8150, Validation Accuracy: 0.7125\n",
      "Epoch 7/30, Validation Loss: 0.7686, Validation Accuracy: 0.7384\n",
      "Epoch 8/30, Validation Loss: 0.7521, Validation Accuracy: 0.7389\n",
      "Epoch 9/30, Validation Loss: 0.7397, Validation Accuracy: 0.7510\n",
      "Epoch 10/30, Validation Loss: 0.7504, Validation Accuracy: 0.7498\n",
      "Epoch 11/30, Validation Loss: 0.7500, Validation Accuracy: 0.7530\n",
      "Epoch 12/30, Validation Loss: 0.7759, Validation Accuracy: 0.7552\n",
      "Epoch 13/30, Validation Loss: 0.7872, Validation Accuracy: 0.7620\n",
      "Epoch 14/30, Validation Loss: 0.7917, Validation Accuracy: 0.7643\n",
      "Epoch 15/30, Validation Loss: 0.8381, Validation Accuracy: 0.7622\n",
      "Epoch 16/30, Validation Loss: 0.9059, Validation Accuracy: 0.7541\n",
      "Epoch 17/30, Validation Loss: 0.8763, Validation Accuracy: 0.7713\n",
      "Epoch 18/30, Validation Loss: 0.8992, Validation Accuracy: 0.7660\n",
      "Epoch 19/30, Validation Loss: 0.9197, Validation Accuracy: 0.7683\n",
      "Epoch 20/30, Validation Loss: 0.9478, Validation Accuracy: 0.7659\n",
      "Epoch 21/30, Validation Loss: 0.9618, Validation Accuracy: 0.7696\n",
      "Epoch 22/30, Validation Loss: 0.9832, Validation Accuracy: 0.7702\n",
      "Epoch 23/30, Validation Loss: 0.9648, Validation Accuracy: 0.7750\n",
      "Epoch 24/30, Validation Loss: 1.0149, Validation Accuracy: 0.7710\n",
      "Epoch 25/30, Validation Loss: 1.0120, Validation Accuracy: 0.7750\n",
      "Epoch 26/30, Validation Loss: 1.0026, Validation Accuracy: 0.7762\n",
      "Epoch 27/30, Validation Loss: 1.0521, Validation Accuracy: 0.7641\n",
      "Epoch 28/30, Validation Loss: 1.0445, Validation Accuracy: 0.7769\n",
      "Epoch 29/30, Validation Loss: 1.0727, Validation Accuracy: 0.7731\n",
      "Epoch 30/30, Validation Loss: 1.0306, Validation Accuracy: 0.7806\n",
      "Treinamento concluído.\n",
      "Avaliação final - Loss: 1.0306, Accuracy: 0.7806\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Função para criar um subconjunto dos dados\n",
    "def create_subset(dataset, subset_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    subset_indices = random.sample(indices, subset_size)\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "\n",
    "# Função para selecionar o modelo sem pré-treinamento\n",
    "def select_model(architecture):\n",
    "    if architecture == 'alexnet':\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'vgg11':\n",
    "        model = models.vgg11(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif architecture == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(weights=None)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
    "    else:\n",
    "        raise ValueError(\"Arquitetura não suportada: escolha 'alexnet', 'vgg11', 'resnet18' ou 'mobilenet_v2'\")\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Transformações de dados com normalizações para modelos pré-treinados\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Reduzir a quantidade de dados usados\n",
    "train_subset_size = 50000  # Número ainda mais reduzido de exemplos de treino\n",
    "test_subset_size = 10000    # Número ainda mais reduzido de exemplos de teste\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_subset = create_subset(trainset, train_subset_size)\n",
    "trainloader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_subset = create_subset(testset, test_subset_size)\n",
    "testloader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "# Carregar o conjunto de dados CIFAR-10\n",
    "#trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criar DataLoader\n",
    "#trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "#testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Função para treinamento\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(trainloader)\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    print('Treinamento concluído.')\n",
    "    return model\n",
    "\n",
    "# Função para avaliação\n",
    "def evaluate_model(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Definir arquitetura e otimizador\n",
    "architecture = 'mobilenet_v2'  # Pode ser 'alexnet', 'vgg11', 'resnet18' ou 'mobilenet_v2'\n",
    "optimizer_choice = 'Adam'  # Escolha entre 'AdaGrad', 'RMSProp' ou 'Adam'\n",
    "\n",
    "model = select_model(architecture)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Selecionar o otimizador\n",
    "if optimizer_choice == 'AdaGrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "elif optimizer_choice == 'RMSProp':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "elif optimizer_choice == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "else:\n",
    "    raise ValueError(\"Otimizador não suportado: escolha 'AdaGrad', 'RMSProp' ou 'Adam'\")\n",
    "\n",
    "# Treinar o modelo\n",
    "num_epochs = 30\n",
    "trained_model = train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Avaliação final\n",
    "final_loss, final_accuracy = evaluate_model(trained_model, criterion)\n",
    "print(f'Avaliação final - Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "torch.save(trained_model.state_dict(), f'trained_model_{architecture}_{optimizer_choice}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zB8QPi88KZPK",
   "metadata": {
    "id": "zB8QPi88KZPK"
   },
   "source": [
    "## SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hSDHfJ1-KTXp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 728817,
     "status": "error",
     "timestamp": 1729902835337,
     "user": {
      "displayName": "Eliezer Sanhá",
      "userId": "07034799936810465209"
     },
     "user_tz": 180
    },
    "id": "hSDHfJ1-KTXp",
    "outputId": "b915109e-625f-4ba2-970b-d62b9140702e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/30, Validation Loss: 1.5549, Validation Accuracy: 0.4204\n",
      "Epoch 2/30, Validation Loss: 1.3613, Validation Accuracy: 0.4974\n",
      "Epoch 3/30, Validation Loss: 1.2883, Validation Accuracy: 0.5267\n",
      "Epoch 4/30, Validation Loss: 1.2692, Validation Accuracy: 0.5462\n",
      "Epoch 5/30, Validation Loss: 1.1594, Validation Accuracy: 0.5784\n",
      "Epoch 6/30, Validation Loss: 1.0910, Validation Accuracy: 0.6134\n",
      "Epoch 7/30, Validation Loss: 1.0642, Validation Accuracy: 0.6175\n",
      "Epoch 8/30, Validation Loss: 0.9982, Validation Accuracy: 0.6422\n",
      "Epoch 9/30, Validation Loss: 1.0222, Validation Accuracy: 0.6376\n",
      "Epoch 10/30, Validation Loss: 0.9640, Validation Accuracy: 0.6600\n",
      "Epoch 11/30, Validation Loss: 0.9008, Validation Accuracy: 0.6828\n",
      "Epoch 12/30, Validation Loss: 0.8887, Validation Accuracy: 0.6849\n",
      "Epoch 13/30, Validation Loss: 0.8800, Validation Accuracy: 0.6874\n",
      "Epoch 14/30, Validation Loss: 0.8368, Validation Accuracy: 0.7020\n",
      "Epoch 15/30, Validation Loss: 0.8939, Validation Accuracy: 0.6876\n",
      "Epoch 16/30, Validation Loss: 0.8403, Validation Accuracy: 0.7032\n",
      "Epoch 17/30, Validation Loss: 0.8082, Validation Accuracy: 0.7227\n",
      "Epoch 18/30, Validation Loss: 0.7887, Validation Accuracy: 0.7254\n",
      "Epoch 19/30, Validation Loss: 0.7974, Validation Accuracy: 0.7278\n",
      "Epoch 20/30, Validation Loss: 0.7269, Validation Accuracy: 0.7443\n",
      "Epoch 21/30, Validation Loss: 0.7774, Validation Accuracy: 0.7348\n",
      "Epoch 22/30, Validation Loss: 0.7807, Validation Accuracy: 0.7310\n",
      "Epoch 23/30, Validation Loss: 0.7847, Validation Accuracy: 0.7334\n",
      "Epoch 24/30, Validation Loss: 0.7485, Validation Accuracy: 0.7426\n",
      "Epoch 25/30, Validation Loss: 0.7007, Validation Accuracy: 0.7591\n",
      "Epoch 26/30, Validation Loss: 0.7216, Validation Accuracy: 0.7511\n",
      "Epoch 27/30, Validation Loss: 0.6986, Validation Accuracy: 0.7648\n",
      "Epoch 28/30, Validation Loss: 0.7281, Validation Accuracy: 0.7497\n",
      "Epoch 29/30, Validation Loss: 0.6444, Validation Accuracy: 0.7823\n",
      "Epoch 30/30, Validation Loss: 0.6679, Validation Accuracy: 0.7756\n",
      "Treinamento concluído.\n",
      "Avaliação final - Loss: 0.6679, Accuracy: 0.7756\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Função para criar um subconjunto dos dados\n",
    "def create_subset(dataset, subset_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    subset_indices = random.sample(indices, subset_size)\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "\n",
    "# Função para selecionar o modelo sem pré-treinamento\n",
    "def select_model(architecture):\n",
    "    if architecture == 'alexnet':\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'vgg11':\n",
    "        model = models.vgg11(weights=None)\n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
    "    elif architecture == 'resnet18':\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif architecture == 'squeezenet':\n",
    "        model = models.squeezenet1_1(weights=None)\n",
    "        model.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
    "        model.num_classes = 10\n",
    "    else:\n",
    "        raise ValueError(\"Arquitetura não suportada: escolha 'alexnet', 'vgg11', 'resnet18' ou 'squeezenet'\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "# Transformações de dados com normalizações para modelos pré-treinados\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Reduzir a quantidade de dados usados\n",
    "train_subset_size = 50000  # Número ainda mais reduzido de exemplos de treino\n",
    "test_subset_size = 10000    # Número ainda mais reduzido de exemplos de teste\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_subset = create_subset(trainset, train_subset_size)\n",
    "trainloader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_subset = create_subset(testset, test_subset_size)\n",
    "testloader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=2)  # Reduzir o batch_size\n",
    "\n",
    "# Carregar o conjunto de dados CIFAR-10\n",
    "#trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criar DataLoader\n",
    "#trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "#testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Função para treinamento\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(trainloader)\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    print('Treinamento concluído.')\n",
    "    return model\n",
    "\n",
    "# Função para avaliação\n",
    "def evaluate_model(model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Definir arquitetura e otimizador\n",
    "architecture = 'squeezenet'  # Escolha entre 'alexnet', 'vgg11', 'resnet18' ou 'squeezenet'\"\n",
    "optimizer_choice = 'Adam'  # Escolha entre 'AdaGrad', 'RMSProp' ou 'Adam'\n",
    "\n",
    "model = select_model(architecture)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Selecionar o otimizador\n",
    "if optimizer_choice == 'AdaGrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "elif optimizer_choice == 'RMSProp':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "elif optimizer_choice == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "else:\n",
    "    raise ValueError(\"Otimizador não suportado: escolha 'AdaGrad', 'RMSProp' ou 'Adam'\")\n",
    "\n",
    "# Treinar o modelo\n",
    "num_epochs = 30\n",
    "trained_model = train_model(model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Avaliação final\n",
    "final_loss, final_accuracy = evaluate_model(trained_model, criterion)\n",
    "print(f'Avaliação final - Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}')\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "torch.save(trained_model.state_dict(), f'trained_model_{architecture}_{optimizer_choice}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5781dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
